{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siahw/notes/blob/main/%E2%80%9CAssignment2_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0654ab8c",
      "metadata": {
        "id": "0654ab8c"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "- In this assignment, you will train a model for sentiment analysis\n",
        "    - Sentiment analysis is to predict whether the given text's sentiment is positive or negative\n",
        "    - The input is a sequence of tokens and the output is a result of sigmoid function\n",
        "- You have to submit a report (in pdf) and your code (in ipynb)\n",
        "    - In your report, you have to briefly explain about your code\n",
        "        - Code explanation can be very simple\n",
        "    - Also, add explanation on these problems\n",
        "        - Problem 2: Explanation on a given lines of code\n",
        "        - Problem 5: Result of training\n",
        "        - Problem 6: Analyze the Prediction of Model \n",
        "- The main goal of this assignment is to implement a pipeline to train a neural network\n",
        "    - Problem 1: Building a Dataset class (9 pts)\n",
        "    - Problem 2: Build a Str2Idx2Str Converter (8 pts)\n",
        "        - Complete thefunction\n",
        "        - Describe additional lines in your report\n",
        "    - Problem 3: Implement a collate function (8 pts)\n",
        "    - Problem 4: Implement a Binary Cross Entropy Loss (5 pts)\n",
        "    - Problem 5: Complete Training Loop (10 pts)\n",
        "        - Training with a single batch\n",
        "        - Validate the model\n",
        "    - Problem 6: Analyze the Prediction of Model (20 pts)\n",
        "        - Write it in your report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737b088c",
      "metadata": {
        "id": "737b088c"
      },
      "source": [
        "## Preparation: Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5531281c",
      "metadata": {
        "id": "5531281c"
      },
      "outputs": [],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f68924c",
      "metadata": {
        "id": "2f68924c"
      },
      "outputs": [],
      "source": [
        "!tar -xzf aclImdb_v1.tar.gz # unzip the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937695d9",
      "metadata": {
        "id": "937695d9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "import gensim.downloader\n",
        "wrd2vec_model = gensim.downloader.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b26a09b",
      "metadata": {
        "id": "8b26a09b"
      },
      "source": [
        "### Make Datasplit\n",
        "- In typical machine learning tasks, one has to split training set and validation set\n",
        "    - Training set is to train the model's parameter\n",
        "    - Validation set is to check how model works for unseen dataset\n",
        "        - Validation set is used to optimize model's hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab60c3b3",
      "metadata": {
        "id": "ab60c3b3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You don't have to change this cell\n",
        "'''\n",
        "\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "train_path = Path('aclImdb/train')\n",
        "test_path = Path('aclImdb/test')\n",
        "\n",
        "def get_train_txt_paths_in_split(dir_path='aclImdb/train'):\n",
        "  dir_path = Path(dir_path)\n",
        "  train_set, valid_set = [], []\n",
        "  random.seed(0) # manually seed random so that you can get the same random result whenever you run the code for reproducibility\n",
        "  for typ in ('pos', 'neg'):\n",
        "    paths_of_typ = list( (train_path / typ).glob('*.txt'))\n",
        "    num_examples = len(paths_of_typ)\n",
        "    num_train_sample = num_examples * 4 // 5\n",
        "    \n",
        "    paths_of_typ = sorted(paths_of_typ)\n",
        "    random.shuffle(paths_of_typ) # shuffle the dataset\n",
        "    train_set += paths_of_typ[:num_train_sample] # assign first num_train_sample samples for train set\n",
        "    valid_set += paths_of_typ[num_train_sample:] # assign the remaining samples for validation set\n",
        "    \n",
        "  random.shuffle(train_set)\n",
        "  random.shuffle(valid_set)\n",
        "  \n",
        "  return train_set, valid_set\n",
        "\n",
        "\n",
        "train_pths, valid_pths = get_train_txt_paths_in_split('aclImdb/train')\n",
        "test_pths = list(test_path.rglob(\"*.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ad6145",
      "metadata": {
        "id": "42ad6145",
        "outputId": "58c65597-1bed-4906-e1c8-761ce380b6d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('aclImdb/train/neg/6600_1.txt'),\n",
              " PosixPath('aclImdb/train/neg/9945_1.txt'),\n",
              " PosixPath('aclImdb/train/pos/7684_7.txt'),\n",
              " PosixPath('aclImdb/train/pos/2614_9.txt'),\n",
              " PosixPath('aclImdb/train/pos/3662_9.txt'),\n",
              " PosixPath('aclImdb/train/pos/2431_8.txt'),\n",
              " PosixPath('aclImdb/train/pos/8332_9.txt'),\n",
              " PosixPath('aclImdb/train/neg/992_1.txt'),\n",
              " PosixPath('aclImdb/train/pos/8901_8.txt'),\n",
              " PosixPath('aclImdb/train/pos/1301_10.txt')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        " Print the first 10 paths in train_pths\n",
        "'''\n",
        "train_pths[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b8f868",
      "metadata": {
        "id": "69b8f868"
      },
      "source": [
        "## Problem 1: Complete the dataset class\n",
        "- Complete the given class ``IMDbData`` \n",
        "    - ``IMDbData`` has a list of txt paths. Each txt corresponds to a single data sample.\n",
        "        - **The label, whether the given sentence is positive or negative, is recorded in the name of directory path of the file**\n",
        "        - You can convert ``Path`` instance to ``str`` by ``str(a_path)``\n",
        "    - Complete two special methods ``__len__`` and ``__getitem__``\n",
        "        - ``__len__`` returns the length of the dataset, which is number of total data samples in the dataset\n",
        "        - ``__getitem__`` takes an index and returns the corresponding data sample for a given index\n",
        "        - To read txt file, you can use a pre-defined ``read_txt`` function\n",
        "            - ``read_txt`` gets an txt file path as an input and returns a content of the txt file in a string\n",
        "        - To get a list of token, use ``self.tokenizer``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed1a575",
      "metadata": {
        "id": "6ed1a575"
      },
      "outputs": [],
      "source": [
        "def read_txt(txt_path):\n",
        "  with open(txt_path, 'r') as f:\n",
        "    txt_string = f.readline()\n",
        "  return txt_string\n",
        "\n",
        "class IMDbData:\n",
        "  def __init__(self, path_list):\n",
        "    self.paths = path_list\n",
        "    self.tokenizer = get_tokenizer('basic_english')\n",
        "  \n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    __len__ is a special method that returns length of the instance when called with len(class_instance)\n",
        "    e.g.\n",
        "      dataset = IMDbData()\n",
        "      length_of_dataset = len(dataset)\n",
        "      \n",
        "    TODO: Complete this function \n",
        "    \"\"\"\n",
        "    return \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    __getitem__ is a special method that returns an item for a given index when called with class_instance[index]\n",
        "    e.g.\n",
        "      trainset = IMDbData(train_pths)\n",
        "      trainset[6] == trainset.__getitem__(6)\n",
        "      \n",
        "    output: sequence_of_token, label\n",
        "      sequence_of_token (list): a list of string (word token). Use self.tokenizer to make string into a list of word token\n",
        "      label (int): 0 if the sentence is negative, 1 if the sentence is positive    \n",
        "      \n",
        "    HINT: use str(pth) to convert Path into String.\n",
        "          You can find the label of the sample in its file directory path\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    TODO: Complete this function\n",
        "    \"\"\"\n",
        "\n",
        "    return \n",
        "\n",
        "trainset = IMDbData(train_pths)\n",
        "validset = IMDbData(valid_pths)\n",
        "short_validset = IMDbData(valid_pths[:100])\n",
        "testset = IMDbData(test_pths)\n",
        "\n",
        "# print('__len__ result for trainset: ', len(trainset))\n",
        "# print('__getitem__ result: ', trainset[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345935b6",
      "metadata": {
        "id": "345935b6",
        "outputId": "0293292b-1636-42d9-f986-24a58cd85c63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['this', 'is', 'my', 'example', 'sentence', '!', '!']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Examples of how tokenizer works\n",
        "'''\n",
        "trainset.tokenizer('this is my example Sentence!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73b5f1a",
      "metadata": {
        "id": "d73b5f1a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test your IMDbData class\n",
        "'''\n",
        "trainset = IMDbData(train_pths)\n",
        "assert len(trainset) == 20000 and len(validset) ==5000 and len(short_validset)==100\n",
        "assert len(trainset[0]) == 2\n",
        "assert trainset[154][0][10:15] == ['ends', 'right', 'after', 'this', 'little'], \"Error in the trainset __getitem__ output\"\n",
        "assert trainset[594][1] == 0 and trainset[523][1] == 1 and trainset[1523][1] == 0, \"Error in the trainset __getitem__ output\"\n",
        "\n",
        "print(\"Passed all the test cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3e235a",
      "metadata": {
        "id": "0a3e235a"
      },
      "source": [
        "## Problem 2: Complete String to idx Converter\n",
        "- Complete a class for converting a list of string to a list of integer\n",
        "    - Important HINT: Use ``word2vec.index_to_key``, and slice so that only vocab_size words are included in the ``self.idx2str``\n",
        "- **In your report, you have to explain why you need ``self.unknown_idx`` and ``self.idx2str.append(\"UNKNOWN\")``**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbb6fb1",
      "metadata": {
        "id": "2cbb6fb1",
        "outputId": "269dec90-0908-4a09-8e45-075db8d39b21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Play with wrd2vec\n",
        "'''\n",
        "wrd2vec_model.index_to_key[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dac770a",
      "metadata": {
        "id": "1dac770a"
      },
      "outputs": [],
      "source": [
        "class Str2Idx2Str:\n",
        "  def __init__(self, wrd2vec, vocab_size=30000):\n",
        "    '''\n",
        "    TODO: Complete the class\n",
        "    \n",
        "    1. Declare self.idx2str\n",
        "    - self.idx2str is a list of strings, which contains a string of word that corresponds to the index of the list\n",
        "    - e.g. self.idx2str[your_index] returns a string value of your_index of the vocabulary\n",
        "    - Use wrd2vec.index_to_key, and slice so that only vocab_size words are included in the idx2str\n",
        "    \n",
        "    2. Declare self.str2idx\n",
        "    - self.str2idx is a dictionary, where its keys are the words in strings and its values are the corresponding index of each word\n",
        "    - e.g. self.str2idx[your_word] returns an integer value of the index of your_word in the vocabulary\n",
        "\n",
        "    '''\n",
        "    self.idx2str = []\n",
        "    self.str2idx = {}\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    You have to add these lines,\n",
        "    And explain in your report what is the function of these two lines \n",
        "    '''\n",
        "    self.unknown_idx = len(self.str2idx)\n",
        "    self.idx2str.append(\"UNKNOWN\")\n",
        "  \n",
        "    '''\n",
        "    Below is the assertion \n",
        "    '''\n",
        "    assert len(self.idx2str) == vocab_size+1, \"Size of the vocabulary has to be vocab_size + 1 (for unknown)\"\n",
        "    \n",
        "  \n",
        "  def __call__(self, alist):\n",
        "    '''\n",
        "    This function converts list of word string to its index and vice versa.\n",
        "    For example, if it takes ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you'] as an input,\n",
        "    it will return [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81].\n",
        "    \n",
        "    If it takes [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81],\n",
        "    it will return ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
        "    \n",
        "    If it takes a list of list of string, such as [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']],\n",
        "    it will return a list of list of integer, [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]]\n",
        "    \n",
        "    Vice versa, if it takes [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]] as an input,\n",
        "    it will return [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']],\n",
        "    \n",
        "    Input: alist of strings, or a list of integers, or a list of lists\n",
        "      e.g. alist = ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
        "        or alist = [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81]\n",
        "        or alist = [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']]\n",
        "        or alist = [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]]\n",
        "    \n",
        "    IMPORTANT: If a word in the input list is not in the vocabulary of Str2Idx2Str, then it has to convert it into UNKNOWN token.\n",
        "    \n",
        "    \n",
        "    Output: a list of integer\n",
        "    \n",
        "    TODO: Complete this function, using self.idx2str and self.str2idx\n",
        "    \n",
        "    Hint: You can figure out the type of input by using the function isinstance. It will return boolean.\n",
        "        isinstance(an_item, list)\n",
        "        isinstance(an_item, str)\n",
        "        isinstance(an_item, int)\n",
        "    '''\n",
        "    \n",
        "    # Write your code from here\n",
        "    \n",
        "    \n",
        "    return\n",
        "\n",
        "# Test the code\n",
        "converter = Str2Idx2Str(wrd2vec_model, vocab_size=30000)\n",
        "input_sentence = trainset[0][0][:20] #0th sample, text (instead of label), first 20 words\n",
        "print(f\"Input sentence: {input_sentence}\")\n",
        "print(f\"Converted sentence: {converter(input_sentence)}\")\n",
        "print(f\"Re-converted sentence: {converter(converter(input_sentence))}\")\n",
        "print(f\"Result for a list of sentences/ input_list: {[trainset[i][0][:5]for i in range(1,5)]}, output_list: {converter([trainset[i][0][:5]for i in range(1,5)])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4edac0e1",
      "metadata": {
        "id": "4edac0e1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test your code by running this cell.\n",
        "\n",
        "Don't change the test cases\n",
        "'''\n",
        "\n",
        "list_of_string = ['if', 'anyone', 'who', 'loves', 'laurel', 'and', 'hardy', 'can', 'watch', 'this', 'movie', 'and', 'feel', 'good', 'about', 'it', ',', 'you']\n",
        "list_of_intger = [83, 1544, 38, 6741, 15722, 5, 10801, 86, 1716, 37, 1005, 5, 998, 219, 59, 20, 1, 81]\n",
        "\n",
        "assert converter(list_of_string) == list_of_intger, \\\n",
        "    f\"The output of converting list_of_string has to be same with list_of_intger. Your current output is {converter(list_of_string)}\"\n",
        "assert converter(list_of_intger) == list_of_string, \\\n",
        "    f\"The output of converting list_of_intger has to be same with list_of_string. Your current output is {converter(list_of_intger)}\"\n",
        "\n",
        "\n",
        "list_of_string_list = [['after', 'watching', 'about', 'half', 'of'], ['reading', 'all', 'of', 'the', 'comments'], ['why', 'has', 'this', 'not', 'been'], ['this', 'is', 'a', 'really', 'strange']]\n",
        "list_of_integer_list = [[49, 2641, 59, 343, 3], [2185, 64, 3, 0, 1939], [738, 31, 37, 36, 51], [37, 14, 7, 588, 5186]]\n",
        "\n",
        "assert converter(list_of_string_list) == list_of_integer_list, \\\n",
        "    f\"The output of converting list_of_string_list has to be same with list_of_integer_list. Your current output is {converter(list_of_string_list)}\"\n",
        "assert converter(list_of_integer_list) == list_of_string_list, \\\n",
        "    f\"The output of converting list_of_integer_list has to be same with list_of_string_list. Your current output is {converter(list_of_integer_list)}\"\n",
        "\n",
        "print(\"Passed all the test cases!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0274efa4",
      "metadata": {
        "id": "0274efa4"
      },
      "source": [
        "## Problem 3: Complete Collate Function\n",
        "- Every data sample in ``IMDbData`` has different length\n",
        "    - Therefore, you have to handle various input length to group the multiple sequnece samples as a tensor\n",
        "- You have to implement ``pack_collate`` which takes a raw batch from the dataset and groups it into a ``PackedSequence``\n",
        "    - You don't need to know about ``PackedSequence`` now. It helps to implement an efficient computation for sequence with different lengths\n",
        "- Implement three variables, following the description in the function\n",
        "    - ``txts_in_idxs``, ``labels``, ``lengths``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85545f25",
      "metadata": {
        "id": "85545f25"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence, pad_packed_sequence\n",
        "import torch\n",
        "\n",
        "def pack_collate(batch):\n",
        "  '''\n",
        "  TODO: Declare variables txts_in_idxs, label_tensor, and lengths, following the description below\n",
        "  \n",
        "  word_sentences_in_idxs: A list of torch.LongTensor. Each element in a list is a sequence of integer, and the each integer represents a vocabulary index of word in a sentence.\n",
        "                i-th element of word_sentences_in_idxs corresponds to the i-th data sample in the batch  \n",
        "  labels: torch.FloatTensor with a shape of [len(batch)]. i-th value of the tensor represents the label of the i-th data sample in the batch (either 0.0 or 1.0)\n",
        "  lengths_of_sentences: A list of integer. i-th value of lengths represents the sequence length (number of word tokens) of the i-th data sample in the batch\n",
        "  '''\n",
        "  \n",
        "  # Write your code from here\n",
        "  word_sentences_in_idxs = []\n",
        "  lengths_of_sentences = []\n",
        "#   label_tensor = torch.Tensor([])\n",
        "  \n",
        "  '''\n",
        "  Leave the code below as it is\n",
        "  '''\n",
        "  assert isinstance(word_sentences_in_idxs, list), f\"txts_in_idxs has to be a list, not {type(word_sentences_in_idxs)}\"\n",
        "  assert isinstance(word_sentences_in_idxs[0], torch.LongTensor), f\"An elmenet of txts_in_idxs has to be a torch.LongTensor, not {type(word_sentences_in_idxs[0])}\"\n",
        "  assert isinstance(label_tensor, torch.FloatTensor), f\"labels has to be a torch.FloatTensor, not {type(labels)}\"\n",
        "  assert label_tensor[-1] == batch[-1][1], \"i-th element of labels has to be \"\n",
        "  assert isinstance(lengths_of_sentences, list), f\"lengths_of_sentences has to be a list, not {type(lengths_of_sentences)}\"\n",
        "  assert lengths_of_sentences[-1] == len(batch[-1][0]), \"i-th value of lengths_of_sentences has to be same with the sentence of i-th data sample\"\n",
        "  assert len(label_tensor) == len(lengths_of_sentences) == len(batch), \"Length of labels, lengths, and batch has to be the same\"\n",
        "  \n",
        "  padded_sequence = pad_sequence(word_sentences_in_idxs)\n",
        "  packed_sequence = pack_padded_sequence(padded_sequence, lengths=lengths_of_sentences, enforce_sorted=False)\n",
        "  \n",
        "  return packed_sequence, label_tensor\n",
        "\n",
        "# Test the code\n",
        "train_loader = DataLoader(trainset, batch_size=32, collate_fn=pack_collate, shuffle=True)\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print('A batch looks like this: ', batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc96e0d1",
      "metadata": {
        "id": "fc96e0d1"
      },
      "source": [
        "### Preparation: Define Model\n",
        "- You don't have to change this code, or try to understand how this GRU model works at the current stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa84a216",
      "metadata": {
        "id": "fa84a216"
      },
      "outputs": [],
      "source": [
        "class SentimentModel(nn.Module):\n",
        "  def __init__(self, wrd2vec, hidden_size=128, num_layers=3, vocab_size=30000):\n",
        "    super().__init__()\n",
        "    self.word_embedding = nn.Embedding(vocab_size+1, 300)\n",
        "    self.word_embedding.weight.data[:vocab_size] = torch.Tensor(wrd2vec.vectors[:vocab_size])\n",
        "    self.gru = nn.GRU(300, hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.3)\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.final_layer = nn.Linear(hidden_size*2, 1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = PackedSequence(self.word_embedding(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    x, hidden = self.gru(x)\n",
        "    pad_x, lens = pad_packed_sequence(x)\n",
        "    max_x = torch.max(pad_x, dim=0)[0]\n",
        "    pred_logit = self.final_layer(max_x)[:,0]\n",
        "    return torch.sigmoid(pred_logit)\n",
        "  \n",
        "# Test the model\n",
        "model = SentimentModel(wrd2vec_model)\n",
        "batch = next(iter(train_loader))\n",
        "x, y = batch\n",
        "out = model(x)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8e45ef",
      "metadata": {
        "id": "1e8e45ef"
      },
      "source": [
        "## Problem 4: Implement Binary Cross Entropy Loss\n",
        "- Without using ``torch.nn.BCELoss``\n",
        "    - You can implement it with ``torch.log`` and ``torch.mean`` or ``atensor.mean()``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e39022",
      "metadata": {
        "id": "90e39022"
      },
      "outputs": [],
      "source": [
        "def get_binary_cross_entropy_loss(pred, target):\n",
        "  '''\n",
        "  pred (torch.FloatTensor): Prediction value for N samples \n",
        "                            Each element in the tensor is the output of torch.sigmoid, and has a value between 0 and 1\n",
        "  target (torch.FloatTensor): Corresponding target value for N samples. \n",
        "                              Each element in the tensor has value of either 0 or 1\n",
        "  \n",
        "  output: Mean of binary cross entropy of N samples\n",
        "  \n",
        "  TODO: Complete this function\n",
        "  \n",
        "  '''\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99634eb",
      "metadata": {
        "id": "e99634eb"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test your BCE loss function\n",
        "Don't change the test cases\n",
        "'''\n",
        "\n",
        "test_pred_case = torch.Tensor([9.9894e-01, 2.2645e-03, 1.8131e-01, 8.0153e-03, 9.9972e-01, 1.0378e-03,\n",
        "        9.9949e-01, 9.9967e-01, 6.4150e-03, 9.9912e-01, 9.9896e-01, 1.4350e-01,\n",
        "        9.9896e-01, 2.1979e-02, 9.9976e-01, 4.5389e-03, 9.9906e-01, 1.0633e-02,\n",
        "        9.9749e-01, 5.5501e-04, 7.0052e-04, 2.9509e-04, 3.2752e-04, 9.9940e-01,\n",
        "        4.5912e-04, 9.9969e-01, 6.0225e-03, 9.9974e-01, 9.9907e-01, 9.9942e-01,\n",
        "        4.0911e-01, 2.8850e-01])\n",
        "test_target_case = torch.Tensor([1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
        "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.])\n",
        "\n",
        "your_result = get_binary_cross_entropy_loss(test_pred_case, test_target_case)\n",
        "\n",
        "'''\n",
        "The value can be little different because of epsilon value used for torch.log\n",
        "'''\n",
        "print(f\"BCE Loss by torch.nn.BCELoss is {torch.nn.BCELoss()(test_pred_case, test_target_case)} and your BCE loss is {your_result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2497890e",
      "metadata": {
        "id": "2497890e"
      },
      "source": [
        "### Problem 5: Complete Training Loop\n",
        "- In this problem, you have to implement a Trainer class\n",
        "    - It contains everything you need to train a neural network model\n",
        "    - model, optimizer, loss function, train loader, validation loader, and device (cuda or cpu)\n",
        "- IMPORTANT\n",
        "    - Select proper ``batch_size`` for ``train_loader``, ``validation_loader``, ``test_loader``\n",
        "\n",
        "- Complete ``_train_by_single_batch``\n",
        "    - You can test it on the cell below\n",
        "- Complete ``validate``\n",
        "    - Implement it with **preventing gradient calculation** to reduce memory usage\n",
        "        - Use ``with torch.no_grad():``\n",
        "    - Calculate Accuracy\n",
        "        - If the prediction is larger than 0.5, you can regard it as positive sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7246fae3",
      "metadata": {
        "id": "7246fae3"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_fn = torch.nn.BCELoss()\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    \n",
        "    self.model.to(device)\n",
        "    \n",
        "    self.best_valid_accuracy = 0\n",
        "    self.device = device\n",
        "    \n",
        "    self.training_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.validation_acc = []\n",
        "\n",
        "  def save_model(self, path='imdb_sentiment_model.pt'):\n",
        "    torch.save({'model':self.model.state_dict(), 'optim':self.optimizer.state_dict()}, path)\n",
        "    \n",
        "  def train_by_num_epoch(self, num_epochs):\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "      self.model.train()\n",
        "      for batch in self.train_loader:\n",
        "        loss_value = self._train_by_single_batch(batch)\n",
        "        self.training_loss.append(loss_value)\n",
        "      self.model.eval()\n",
        "      validation_loss, validation_acc = self.validate()\n",
        "      self.validation_loss.append(validation_loss)\n",
        "      self.validation_acc.append(validation_acc)\n",
        "      \n",
        "      if validation_acc > self.best_valid_accuracy:\n",
        "        print(f\"Saving the model with best validation accuracy: Epoch {epoch+1}, Acc: {validation_acc:.4f} \")\n",
        "        self.save_model('imdb_sentiment_model_best.pt')\n",
        "      else:\n",
        "        self.save_model('imdb_sentiment_model_last.pt')\n",
        "      self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "\n",
        "      \n",
        "  def _train_by_single_batch(self, batch):\n",
        "    '''\n",
        "    This method updates self.model's parameter with a given batch\n",
        "    \n",
        "    batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "    \n",
        "    You have to use variables below:\n",
        "    \n",
        "    self.model (SentimentModel/torch.nn.Module): A neural network model\n",
        "    self.optimizer (torch.optim.adam.Adam): Adam optimizer that optimizes model's parameter\n",
        "    self.loss_fn (function): function for calculating BCE loss for a given prediction and target\n",
        "    self.device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "    The model's parameters, optimizer's steps has to be updated inside this method\n",
        "\n",
        "    TODO: Complete this method \n",
        "    '''\n",
        "\n",
        "    \n",
        "    return\n",
        "\n",
        "    \n",
        "  def validate(self, external_loader=None):\n",
        "    '''\n",
        "    This method calculates accuracy and loss for given data loader.\n",
        "    It can be used for validation step, or to get test set result\n",
        "    \n",
        "    input:\n",
        "      data_loader: If there is no data_loader given, use self.valid_loader as default.\n",
        "      \n",
        "    \n",
        "    output: \n",
        "      validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "      validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "      \n",
        "    TODO: Complete this method \n",
        "\n",
        "    '''\n",
        "    \n",
        "    ### Don't change this part\n",
        "    if external_loader and isinstance(external_loader, DataLoader):\n",
        "      loader = external_loader\n",
        "      print('An arbitrary loader is used instead of Validation loader')\n",
        "    else:\n",
        "      loader = self.valid_loader\n",
        "      \n",
        "    self.model.eval()\n",
        "    \n",
        "    '''\n",
        "    Write your code from here, using loader, self.model, self.loss_fn.\n",
        "    '''\n",
        "\n",
        "  \n",
        "\"\"\"\n",
        "Don't change this part\n",
        "\"\"\"\n",
        "model = SentimentModel(wrd2vec_model, hidden_size=128, num_layers=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "train_loader = DataLoader(trainset, batch_size=32, collate_fn=pack_collate, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(validset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "trainer =  Trainer(model, optimizer, get_binary_cross_entropy_loss, train_loader, valid_loader, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782a04f7",
      "metadata": {
        "id": "782a04f7"
      },
      "source": [
        "#### Check the result\n",
        "- Check your implementation works correctly\n",
        "- Don't change the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8db5dae",
      "metadata": {
        "id": "a8db5dae"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is to test trainer._train_by_single_batch\n",
        "\n",
        "If your code is implemented correctly, the loss will go down for this specific batch\n",
        "\"\"\"\n",
        "\n",
        "trainer.model.train()\n",
        "train_batch = next(iter(trainer.train_loader)) # get a batch from train_loader\n",
        "\n",
        "loss_track = []\n",
        "for _ in range(10):\n",
        "  loss_value = trainer._train_by_single_batch(train_batch) # test the trainer\n",
        "  loss_track.append(loss_value)\n",
        "\n",
        "assert isinstance(loss_value, float) and loss_value > 0,  \"The return of trainer._train_by_single_batch has to be a single float value that is larger than 0\"\n",
        "print(f\"Loss value for 10 repetition for the same training batch is  {loss_track:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9dc9edf",
      "metadata": {
        "id": "e9dc9edf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This code is to test trainer.validate\n",
        "\"\"\"\n",
        "\n",
        "short_valid_loader = DataLoader(short_validset, batch_size=50, collate_fn=pack_collate)\n",
        "\n",
        "validation_loss, validation_acc = trainer.validate(short_valid_loader)\n",
        "assert isinstance(validation_loss, float) and isinstance(validation_acc, float), \"Both return value of trainer.validate has to be float\"\n",
        "assert validation_loss > 0, \"Validation Loss has to be larger than 1\"\n",
        "assert 0 <= validation_acc <= 1, \"Validation Acc has to be between 0 and 1\"\n",
        "\n",
        "print(f\"Valid loss: {validation_loss}, Accuracy: {validation_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab7cd7a",
      "metadata": {
        "id": "1ab7cd7a"
      },
      "source": [
        "### Train the model with the completed Trainer\n",
        "- In your report, attach the result of following cells and describe the training result and test result\n",
        "    - Plot of training and validation loss/acc\n",
        "    - Result of your model on test set\n",
        "\n",
        "- [Optional] You can modify the code to train the model in different ways\n",
        "    - optimizer, batch_size, model_size, num_epcohs, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29853df0",
      "metadata": {
        "scrolled": true,
        "id": "29853df0"
      },
      "outputs": [],
      "source": [
        "trainer.train_by_num_epoch(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3987770b",
      "metadata": {
        "id": "3987770b"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Plot the result after the training\n",
        "'''\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.title(\"Training loss\")\n",
        "plt.plot(trainer.training_loss)\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.title(\"Validation loss by epoch\")\n",
        "plt.plot(trainer.validation_loss)\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.title(\"Validation accuracy by epoch\")\n",
        "plt.plot(trainer.validation_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1eec925",
      "metadata": {
        "id": "e1eec925"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get the test result\n",
        "'''\n",
        "\n",
        "test_loss, test_acc = trainer.validate(test_loader)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672a2ea1",
      "metadata": {
        "id": "672a2ea1"
      },
      "source": [
        "## Problem 6: Analyze the Prediction of the model\n",
        "- In this problem, you have to anlayze the prediction of the model\n",
        "    - You can select among the two models\n",
        "        - Trainer is designed to save two models\n",
        "        - ``imdb_sentiment_model_last.pt`` contains the model weights after the last training epoch\n",
        "        - ``imdb_sentiment_model_best.pt`` contains the model weights after the training epoch with the best validation accuracy\n",
        "     \n",
        "- If you failed to train your model by solving the previous problems, you can download the model\n",
        "- In your report, describe your analysis on how the trained model works on the text\n",
        "    - What is the main criteria for model to decide whether the review is positive or negative?\n",
        "    - When does it make mistakes? When does it make nice predictions? \n",
        "    - Does the converted input text has enough information to classify text compared to the original text?\n",
        "        - Do you see any problems in tokenizing or using UNKNOWN?\n",
        "- You can write the analysis by using only the Test Set samples, or using your own review texts\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe75c333",
      "metadata": {
        "id": "fe75c333"
      },
      "source": [
        "#### Download Model (if you have failed to trained your own)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22533d5d",
      "metadata": {
        "id": "22533d5d",
        "outputId": "e90fda54-d58a-4f73-ced9-7437379ccdea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gdown in /home/teo/.local/lib/python3.8/site-packages (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
            "Requirement already satisfied: filelock in /home/teo/.local/lib/python3.8/site-packages (from gdown) (3.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/teo/.local/lib/python3.8/site-packages (from gdown) (4.10.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/lib/python3/dist-packages (from gdown) (2.22.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/teo/.local/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/teo/.local/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C45RElG8aUMDMFJDtG1DBF6RlKBQ0hXe\n",
            "To: /home/teo/userdata/sg-aat-3020/aat3020-2022Spring/imdb_sentiment_model_best_pretrained.pt\n",
            "100%|████████████████████████████████████████| 119M/119M [00:04<00:00, 27.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C8-3NOi-hs6FGmW0qXLQabtJuO0jqaUU\n",
            "To: /home/teo/userdata/sg-aat-3020/aat3020-2022Spring/imdb_sentiment_model_last_pretrained.pt\n",
            "100%|████████████████████████████████████████| 119M/119M [00:03<00:00, 31.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "If you run this code, you can download the pretrained weight, named\n",
        "\"imdb_sentiment_model_best_pretrained.pt\", and\n",
        "\"imdb_sentiment_model_last_pretrained.pt\"\n",
        "'''\n",
        "\n",
        "!pip install gdown\n",
        "!gdown 1C45RElG8aUMDMFJDtG1DBF6RlKBQ0hXe\n",
        "!gdown 1C8-3NOi-hs6FGmW0qXLQabtJuO0jqaUU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aeaf292",
      "metadata": {
        "id": "5aeaf292"
      },
      "source": [
        "#### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4f40e9",
      "metadata": {
        "id": "bf4f40e9"
      },
      "outputs": [],
      "source": [
        "model = SentimentModel(wrd2vec_model, 128, 3)\n",
        "model.load_state_dict(torch.load('imdb_sentiment_model_last.pt')['model']) # Or imdb_sentiment_model_best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cff2d4d",
      "metadata": {
        "id": "0cff2d4d"
      },
      "source": [
        "#### Check the largest error cases from the Test Set\n",
        "- Following code will print out the error case with the largest errors\n",
        "    - Or data sample with smallest error, with ``largest_loss=False``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02737842",
      "metadata": {
        "id": "02737842"
      },
      "outputs": [],
      "source": [
        "nl = '\\n'\n",
        "def sort_data_idx_by_loss(model, data_loader, device='cuda'):\n",
        "  assert isinstance(data_loader.sampler, torch.utils.data.sampler.SequentialSampler)\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  entire_loss = []\n",
        "  entire_pred = []\n",
        "  loss_fn = torch.nn.BCELoss(reduction='none')\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      x, y =batch\n",
        "      pred = model(x.to(device))\n",
        "      loss = loss_fn(pred, y.to(device))\n",
        "      entire_loss += loss.tolist()\n",
        "      entire_pred += pred.tolist()\n",
        "  sorted_indices = sorted(range(len(entire_loss)),key=entire_loss.__getitem__)\n",
        "  return sorted_indices, entire_loss, entire_pred\n",
        "\n",
        "def print_top_k_loss_case(model, data_loader, k=10, device='cuda', largest_loss=True):\n",
        "  sorted_indices, entire_loss, entire_pred =  sort_data_idx_by_loss(model, data_loader, device)\n",
        "  if largest_loss:\n",
        "    sorted_indices = reversed(sorted_indices[-k:])\n",
        "  else:\n",
        "    sorted_indices = sorted_indices[:k]\n",
        "  \n",
        "  for i, idx in enumerate(sorted_indices):\n",
        "    data_sample = data_loader.dataset[idx]\n",
        "    txt = ' '.join(data_sample[0])\n",
        "    txt_tokenized = data_loader.dataset\n",
        "    print(f\" {i}. Sample index: {idx} - Loss: {entire_loss[idx]:.4f}, Model Prediction: {entire_pred[idx]:.4f}, Correct Label: {data_sample[1]} \\\n",
        "          {nl}  Converted Text: {' '.join(converter(converter(data_sample[0])))} {nl}  Original Text: {read_txt(data_loader.dataset.paths[idx])}\")\n",
        "    \n",
        "'''\n",
        "This will print out top-k most incorrect prediction on test set\n",
        "'''\n",
        "\n",
        "print_top_k_loss_case(model, test_loader, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9291a4e7",
      "metadata": {
        "id": "9291a4e7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will print out top-k most correct prediction on test set\n",
        "'''\n",
        "\n",
        "print_top_k_loss_case(model, test_loader, k=10, largest_loss=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c078561",
      "metadata": {
        "id": "3c078561"
      },
      "source": [
        "#### Test wit your own text input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77e9711",
      "metadata": {
        "id": "d77e9711"
      },
      "outputs": [],
      "source": [
        "def estimate_sentiment_of_given_txt(model, input_text):\n",
        "  model.cpu()\n",
        "  model.eval()\n",
        "  tokenizer = trainset.tokenizer\n",
        "  your_text_in_token = tokenizer(your_text)\n",
        "  lengths=torch.LongTensor([len(your_text_in_token)])\n",
        "  model_input = pack_padded_sequence(torch.LongTensor(converter(your_text_in_token)).unsqueeze(1), lengths=lengths)\n",
        "  prediction = model(model_input)\n",
        "  \n",
        "  return prediction\n",
        "\n",
        "\n",
        "your_text = \"\"\"\n",
        "    This movie is terrific\n",
        "\"\"\"\n",
        "\n",
        "estimate_sentiment_of_given_txt(model, your_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "“Assignment2.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}