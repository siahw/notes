{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siahw/notes/blob/main/%E2%80%9CAssignment3_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303113a3",
      "metadata": {
        "id": "303113a3"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "- In this assignment, you will\n",
        "    - 1) implement a Long Short-term Memory (LSTM)\n",
        "    - 2) and fine tune a Language Model\n",
        "- You have to submit your code ipynb\n",
        "- For the Problem 2, you have to submit a report in pdf and txt file of your training \n",
        "    - Explain why you chose the specific corpus\n",
        "    - Submit some of the interesting examples you made with the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628d37a3",
      "metadata": {
        "id": "628d37a3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9645907e",
      "metadata": {
        "id": "9645907e"
      },
      "source": [
        "## Problem: Implement an LSTM cell (20 pts)\n",
        "- with only ``nn.Linear``, ``nn.Parameter``, ``torch.sigmoid``, and ``torch.tanh``\n",
        "- The forward propagation of LSTM cell can be represented as below\n",
        "    - $i_t = \\sigma_g(W_ix_t+U_ih_{t-1}+b_i)$\n",
        "    - $f_t = \\sigma_g(W_fx_t+U_fh_{t-1}+b_f) $\n",
        "    - $\\tilde{c}_t = \\sigma_c(W_cx_t+U_ch_{t-1}+b_c)$\n",
        "    - $o_t = \\sigma_g(W_ox_t+U_oh_{t-1}+b_o)$\n",
        "    - $c_t = f_t \\otimes c_{t-1} + i_t\\otimes \\tilde{c}_t$\n",
        "    - $h_t = o_t \\otimes \\sigma_c(c_t)$\n",
        "    - $\\sigma_g$: sigmoid function. use ``torch.sigmoid()`` or ``atensor.sigmoid()``\n",
        "    - $\\sigma_c$: hyperbolic tangent function. use ``torch.tanh()`` or ``atensor.tanh()``\n",
        "    - $\\otimes$ represents Hadamard product (element-wise product)\n",
        "        - It means that even though you are multiplying two matrices, you don't multiply as a \"matrix multiplication\" but as a simple multiplication between each element of two matrices in a same position\n",
        "        - This can be represetned as a simple multiplication like ``atensor * another_tensor`` in ``torch``\n",
        "- where the each variable can be explained as below:\n",
        "    - $x_t\\in\\mathbb{R}^d$: input vector to the LSTM unit\n",
        "    - $h_t\\in(-1,1)^h$: hidden state vector also known as output vector of the LSTM unit\n",
        "    - $c_t \\in \\mathbb{R}^h$: cell state vector\n",
        "    - $W\\in\\mathbb{R}^{h\\times d}, U\\in\\mathbb{R}^{h\\times h}$, and $b\\in\\mathbb{R}^h$: weight matrices and bias vector parameters which need to be learned during training\n",
        "    - $i_t\\in(0,1)^h$: input/update gate's activation vector. $h$ represents size of hidden states\n",
        "    - $f_t\\in(0,1)^h$: forget gate's activation vector\n",
        "    - $\\tilde{c}_t\\in(-1,1)^h$: cell input activation vector\n",
        "    - $o_t\\in(0,1)^h$: output gate's activation vector\n",
        "\n",
        "\n",
        "- You can implement LSTM with 8 ``nn.Linear`` modules, to strictly follow the equations above. But to easily compare your implementation with PyTorch's official LSTM implementation, I recommend you to implement it with only two ``nn.Linear`` modules with each of it has a ``bias=True``\n",
        "    - If you carefully look at the equation, you can find you only need one bias, but torch's official implementation uses two biases to match CuDNN compatibility\n",
        "\n",
        "- Hint:\n",
        "    - $Wx+b$ can be implemented by \n",
        "        - ``layer = nn.Linear(in_features=x.shape[-1], out_features=W.shape[0], bias=True)``\n",
        "        - ``out = layer(x)``\n",
        "    - ``bias_1 + bias_2`` can be regarded as a single bias ``bias_12=bias_1+bias2``\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b61c5d",
      "metadata": {
        "scrolled": true,
        "id": "f8b61c5d"
      },
      "outputs": [],
      "source": [
        "class MyLSTM(nn.Module):\n",
        "  def __init__(self, input_size: int, hidden_size: int):\n",
        "    super().__init__()\n",
        "    '''\n",
        "    TODO: Define Weights and Bias of a single-layer, uni-directional LSTM\n",
        "    \n",
        "    Arguments\n",
        "      input_size (int): Num dimension of input x. It is represented as `d` in the equations in the explanation part\n",
        "      hidden_size (int): Num dimension of output h. It is represented as `h` in the equations in the explanation part\n",
        "    \n",
        "    Module Definition\n",
        "      self.weight_ih (nn.Linear): Linear layer that combines Weight matrix [W_i | W_f | W_c | W_o] and bias b\n",
        "      self.weight_hh (nn.Linear): Linear layer that combines Weight matrix [U_i | U_f | U_c | U_o] and bias b\n",
        "    \n",
        "    Implementation Condition: To compare your implementation with the torch's official implementation, please strictly follow the explanation above\n",
        "    '''\n",
        "    \n",
        "    self.weight_ih = nn.Linear(in_features=1, out_features=1, bias=True) # TODO: complete this layer\n",
        "    self.weight_hh = nn.Linear(in_features=1, out_features=1, bias=True) # TODO: complete this layer    \n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def _cal_single_step(self, x_t:torch.Tensor, last_hidden:torch.Tensor, last_cell:torch.Tensor):\n",
        "    '''\n",
        "    Argument:\n",
        "      x_t : input of timestep t. Has a shape of [Num_Batch, Num_input_dim]\n",
        "      last_hidden: hidden state of timestep (t-1). Has a shape of [Num_Batch, Num_hidden_dim]\n",
        "      last_cell: cell state of timestep (t-1). Has a shape of [Num_batch, Num_hidden_dim]\n",
        "      \n",
        "    Output:\n",
        "      updated_hidden (torch.Tensor): hidden state of timestep t. Has a shape of [Num_Batch, Num_hidden_dim]\n",
        "      updated_cell (torch.Tensor): cell state of timestep t. Has a shape of [Num_Batch, Num_hidden_dim]\n",
        "      \n",
        "    TODO: Complete this function using the input arguments and self.weight_ih, self.weight_hh\n",
        "    \n",
        "    '''\n",
        "\n",
        "    \n",
        "    return updated_hidden, updated_cell\n",
        "  \n",
        "  def forward(self, x:torch.Tensor, hidden_and_cell_state:tuple=None):\n",
        "    '''\n",
        "    Argument:\n",
        "      x (torch.Tensor): Input sequence. Has a shape of [Num_Batch, Num_Timestep, Num_input_dim\n",
        "      hidden_and_cell_state (optional, tuple): Hidden state and Cell state of last timestep.\n",
        "      \n",
        "    Return:\n",
        "      output, (last_hidden, last_cell)\n",
        "      Be carefule that you have to return two variables, where the second variable is a tuple of two tensors.\n",
        "      \n",
        "      output (torch.Tensor): Output of LSTM that has a shape of [Batch_Size, Num_Time_Steps, Hidden_State_Size]\n",
        "                             It is the concatenation of output hidden states of every given time steps\n",
        "      last_hidden (torch.Tensor): The hidden state of LSTM after calculating entire time steps. \n",
        "      last_cell (torch.Tensor): LSTM has two types of hidden states, and one is called \"cell state\". \n",
        "      \n",
        "    \n",
        "    TODO: Implement this using a for loop and `self._cal_single_step`.\n",
        "    '''\n",
        "    \n",
        "    # Leave the code below as it is\n",
        "    if hidden_and_cell_state is not None and isinstance(hidden_and_cell_state, tuple):\n",
        "      last_hidden = hidden_and_cell_state[0]\n",
        "      last_cell = hidden_and_cell_state[1]\n",
        "    else:\n",
        "      last_hidden = torch.zeros([x.shape[0], self.hidden_size])\n",
        "      last_cell = torch.zeros([x.shape[0], self.hidden_size])\n",
        "    \n",
        "    '''\n",
        "    Write your code from here\n",
        "    '''\n",
        "    \n",
        "    return output, (last_hidden, last_cell)\n",
        "\n",
        "  \n",
        "input_size = 16\n",
        "hidden_size = 32\n",
        "\n",
        "model = MyLSTM(input_size, hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540d8154",
      "metadata": {
        "id": "540d8154"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Simple test case \n",
        "'''\n",
        "\n",
        "input_size = 16\n",
        "hidden_size = 32\n",
        "\n",
        "model = MyLSTM(input_size, hidden_size)\n",
        "\n",
        "dummy_batch_size = 8\n",
        "dummy_time_steps = 20\n",
        "dummy_input = torch.randn([dummy_batch_size, dummy_time_steps, input_size])\n",
        "\n",
        "output, (last_hidden_state, last_cell_state) = model(dummy_input)\n",
        "\n",
        "assert output.shape[0] == dummy_batch_size, \"0th dimension of output has to be the batch size\"\n",
        "assert output.shape[1] == dummy_time_steps, \"1st dimension of output has to be the time steps\"\n",
        "assert output.shape[2] == hidden_size, \"2nd dimension of output has to be the hidden_size\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6598b925",
      "metadata": {
        "id": "6598b925"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Your LSTM model has to return same output for the script below \n",
        "'''\n",
        "\n",
        "# Without for loop\n",
        "total_output, (last_hidden_state, last_cell_state) = model(dummy_input)\n",
        "\n",
        "hidden_and_cell_state = (torch.zeros([dummy_batch_size, hidden_size]), torch.zeros([dummy_batch_size, hidden_size]))\n",
        "for i in range(dummy_time_steps):\n",
        "  time_step_output, hidden_and_cell_state = model(dummy_input[:,i:i+1], hidden_and_cell_state)\n",
        "  \n",
        "assert (total_output[:,-1:] == time_step_output).all(), 'The LSTM output has to be equal for sliced input using for-loop'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf4cd83",
      "metadata": {
        "id": "ebf4cd83"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The script below tests whether your implementation returns same output with PyTorch's official LSTM implementation\n",
        "\n",
        "To pass this test, you have to make sure that you are using the weight of `self.weight_hh` and `self.weight_ih` as the same order\n",
        "with the official PyTorch LSTM\n",
        "\n",
        "Be careful on the order of W_i, W_f, W_c, W_o in the combined weight matrix, when you declare your LSTM's forward propagation!\n",
        "\n",
        "self.weight_ih (nn.Linear): Linear layer that combines Weight matrix [W_i | W_f | W_c | W_o] and bias b\n",
        "self.weight_hh (nn.Linear): Linear layer that combines Weight matrix [U_i | U_f | U_c | U_o] and bias b\n",
        "'''\n",
        "\n",
        "\n",
        "lstm_pre_impl = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "lstm_pre_impl.weight_hh_l0.data = model.weight_hh.weight.data\n",
        "lstm_pre_impl.bias_hh_l0.data = model.weight_hh.bias.data\n",
        "\n",
        "lstm_pre_impl.weight_ih_l0.data = model.weight_ih.weight.data\n",
        "lstm_pre_impl.bias_ih_l0.data = model.weight_ih.bias.data\n",
        "\n",
        "\n",
        "output, (last_hidden_state, last_cell_state) = model(dummy_input)\n",
        "output_compare, (last_hidden_state_compare, last_cell_state_compare) = lstm_pre_impl(dummy_input)\n",
        "\n",
        "assert (output==output_compare).all(), \"The output of LSTM is different\"\n",
        "assert (last_hidden_state==last_hidden_state_compare).all(), \"The last hidden state of LSTM is different\"\n",
        "assert (last_cell_state==last_cell_state_compare).all(), \"The last cell state of LSTM is different\"\n",
        "\n",
        "\n",
        "print(\"Test passed! Your LSTM implementation returns the exactly same result for PyTorch's official implementation of single-layer uni-directiona LSTM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53bfcc6",
      "metadata": {
        "id": "b53bfcc6"
      },
      "source": [
        "## Problem 2: Fine tune Language model and generate texts\n",
        "- In this problem, we will use ``transformers`` from Hugging Face\n",
        "    - You can download a tokenizer and a pre-trained model from Hugging Face\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179fee55",
      "metadata": {
        "id": "179fee55"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c82f6f",
      "metadata": {
        "id": "73c82f6f"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This will take some minutes to download the model\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93806257",
      "metadata": {
        "id": "93806257"
      },
      "source": [
        "## Problem 2-1: Fine-tune the pre-trained Model with your text (10pts)\n",
        "- The model you just downloaded is pre-trained with huge corpus using GPUs heavily\n",
        "- You can exploit this model for your purpose by fine-tuning the model with a small dataset\n",
        "\n",
        "- **You have to prepare your own `txt` file to use as as a training corpus**\n",
        "    - In your report, describe why you chose this text as a training file\n",
        "        - What did you expect for a model that is fine-tuned with this specific text file?\n",
        "\n",
        "- ``TextSet`` helps to batchify a given corpus of `txt` file\n",
        "    - To fit a corpus into ``TextSet``, you have to put text into a single txt file\n",
        "    - Every line breeak will be replaced with a blank space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86fcf22",
      "metadata": {
        "id": "b86fcf22"
      },
      "outputs": [],
      "source": [
        "def read_txt(txt_path):\n",
        "  with open(txt_path, 'r') as f:\n",
        "    txt_string = f.readlines()\n",
        "  return txt_string\n",
        "\n",
        "class TextSet:\n",
        "  def __init__(self, txt_path, tokenizer, batch_len=128):\n",
        "    corpus_string = read_txt(txt_path)\n",
        "    self.text_in_a_string = \"\".join(corpus_string).replace('\\n', ' ')\n",
        "    \n",
        "    self.tokenizer = tokenizer\n",
        "    self.tokenized_corpus = self.tokenizer(self.text_in_a_string)['input_ids']\n",
        "    self.batch_len = batch_len\n",
        "    \n",
        "    self.tokenized_corpus = self.tokenized_corpus[:len(self.tokenized_corpus)//self.batch_len*self.batch_len]\n",
        "    self.corpus_tensor = self._batchify_text()\n",
        "    \n",
        "  def _batchify_text(self):\n",
        "    entire_corpus_tensor = torch.LongTensor(self.tokenized_corpus)\n",
        "    return entire_corpus_tensor.reshape(-1, self.batch_len)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.corpus_tensor)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.corpus_tensor[idx]\n",
        "\n",
        "'''\n",
        "TODO: select your own txt path\n",
        "'''\n",
        "\n",
        "your_txt_path = \"story_of_your_life.txt\"\n",
        "train_set = TextSet(your_txt_path, tokenizer, batch_len=128)\n",
        "\n",
        "# The code below will show how the tokenizer convert and reconvert the given text\n",
        "train_set[0], train_set.tokenizer.decode(train_set[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7faf00",
      "metadata": {
        "id": "df7faf00"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "define train_loader\n",
        "'''\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_set = TextSet(your_txt_path, tokenizer, batch_len=128)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90a1963",
      "metadata": {
        "id": "c90a1963"
      },
      "source": [
        "- In your report, briefly explain how this code trains a language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a72776",
      "metadata": {
        "id": "75a72776"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "Use CUDA (NVIDIA GPU) to train your model\n",
        "'''\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_record = []\n",
        "model.cuda()\n",
        "num_epochs = 10\n",
        "\n",
        "'''\n",
        "In your report, briefly explain how this training loop trains a language model \n",
        "'''\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  for batch in train_loader:\n",
        "    out = model(batch.cuda()).logits\n",
        "    out = torch.softmax(out, dim=-1)\n",
        "    correct_next_word = batch[:, 1:]\n",
        "    out_flatten = out[:, :-1, :].reshape(-1, out.shape[-1])\n",
        "    correct_next_word = correct_next_word.reshape(-1)\n",
        "\n",
        "    prob_of_correct_word = out_flatten[torch.arange(len(correct_next_word)), correct_next_word]\n",
        "\n",
        "    loss = -torch.log(prob_of_correct_word+1e-6).mean()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    loss_record.append(loss.item())\n",
        "    \n",
        "plt.plot(loss_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7389dcd4",
      "metadata": {
        "id": "7389dcd4"
      },
      "source": [
        "### Problem 2-2: Generate Text with your fine-tuned Model (12 pts)\n",
        "- Here, you have to generate text with the fine-tuned model\n",
        "- You have to provide interesting prompt, that is given as a precedding sequence of token\n",
        "- In your report, introduce interesting examples you have generated\n",
        "    - Why those were interesting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d8c1dc",
      "metadata": {
        "id": "84d8c1dc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Declare generator using HuggingFace library\n",
        "'''\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43101c8",
      "metadata": {
        "scrolled": true,
        "id": "a43101c8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Generate sentences using your own prompt and your max legnth\n",
        "\n",
        "'''\n",
        "\n",
        "your_prompt = \"What words will come next? \"\n",
        "your_max_length = 100\n",
        "your_num_sequences = 5\n",
        "\n",
        "generator(your_prompt, max_length=your_max_length, num_return_sequences=your_num_sequences)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "“Assignment3.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}