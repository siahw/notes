{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siahw/notes/blob/main/%E2%80%9C%E2%80%9CAAT0320_Assignment1_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145148bd",
      "metadata": {
        "id": "145148bd"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "In this assignment, you will explore about word vectors.\n",
        "\n",
        "- Submision: A report in ``PDF``, and your completed notebook file in ``ipynb``\n",
        "    - The assignment will be evalulated mainly with report. So please include every detail you want to present in your report\n",
        "    - Report: Free format. You can copy and paste part of your code for some problems.\n",
        "    - ipynb: Save your notebook (with output of each cell if possible) as ipynb and submit it\n",
        "- Evaluation criteria\n",
        "    - How interesting and original are the presented examples\n",
        "    - How well you describe the reason of success or failure of your examples by considering how Word2Vec is trained"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ee4fd6",
      "metadata": {
        "id": "97ee4fd6"
      },
      "source": [
        "## 0. Setup\n",
        "- Check ``gensim`` library is installed\n",
        "  - This assignment code was designed for ``gensim==4.1.2``\n",
        "- List the downloadable vectors from ``gensim``\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0bbbcb",
      "metadata": {
        "id": "db0bbbcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4537839-a1f4-49cb-955a-9b71cd23b82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "4.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pprint as pp\n",
        "print(gensim.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bbeb288",
      "metadata": {
        "id": "9bbeb288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b9b3e1-0fb8-459f-ffbc-71300b506fd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fasttext-wiki-news-subwords-300',\n",
              " 'conceptnet-numberbatch-17-06-300',\n",
              " 'word2vec-ruscorpora-300',\n",
              " 'word2vec-google-news-300',\n",
              " 'glove-wiki-gigaword-50',\n",
              " 'glove-wiki-gigaword-100',\n",
              " 'glove-wiki-gigaword-200',\n",
              " 'glove-wiki-gigaword-300',\n",
              " 'glove-twitter-25',\n",
              " 'glove-twitter-50',\n",
              " 'glove-twitter-100',\n",
              " 'glove-twitter-200',\n",
              " '__testing_word2vec-matrix-synopsis']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "list(gensim.downloader.info()['models'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeae1e1d",
      "metadata": {
        "id": "aeae1e1d"
      },
      "source": [
        "- Among the Word2Vec model codes above, select one model of your choice among ``glove-wiki-gigaword`` or ``glove-twitter``\n",
        "    - numbers at the last represents the number of dimension of each Word2Vec Model\n",
        "        - e.g. ``glove-twitter-25`` was trained on twitter dataset while embedding each word into 25-dim vector\n",
        "        - e.g. ``glove-twitter-200`` was trained on twitter dataset while embedding each word into 200-dim vector\n",
        "- Download the selected model and load it as a ``model``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ace8aa",
      "metadata": {
        "id": "a0ace8aa"
      },
      "outputs": [],
      "source": [
        "your_model_code = 'glove-twitter-25' # select among the model code aboves\n",
        "small_model_code = 'glove-twitter-50'\n",
        "large_model_code = 'glove-twitter-200'\n",
        "\n",
        "small_model = gensim.downloader.load(small_model_code)\n",
        "large_model = gensim.downloader.load(large_model_code)\n",
        "model = gensim.downloader.load(your_model_code) # download and load the model. It can take some time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41125ee7",
      "metadata": {
        "scrolled": true,
        "id": "41125ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b0a435-4efe-4f54-9ce3-d0168b46377d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.1121  , -1.0587  ,  0.11444 ,  0.37088 , -0.62967 ,  0.27092 ,\n",
              "        0.091893,  0.031559,  0.15416 , -0.9241  ,  1.021   ,  0.020354,\n",
              "       -3.3665  , -0.17011 , -0.1616  , -0.27859 , -0.29024 , -0.1468  ,\n",
              "       -0.80652 ,  0.93534 , -0.21278 ,  0.23059 ,  0.51137 , -0.067581,\n",
              "       -0.52111 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# test the model output\n",
        "model['rose']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a673d6",
      "metadata": {
        "id": "73a673d6"
      },
      "source": [
        "## Problem 1. Find Most Similar Words (10 pts)\n",
        "- One of the most simple and typical use case of Word2Vec is finding a word based on similarity.\n",
        "- You can list the most similar words for a given query word by using ``model.most_similar(your_word)``\n",
        "    - Usually, every word in Word2Vec model is in lowercase\n",
        "    \n",
        "- **In your report**, present more than **5** interesting examples and explain why it was interesting for you**\n",
        "    - Try to explain why those words are regarded similar in Word2Vec, considering how it was trained\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccd292a8",
      "metadata": {
        "id": "ccd292a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3ab918-cdee-4731-f740-92d736c33a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('arroz', 0.7267729043960571),\n",
              " ('carne', 0.7077516317367554),\n",
              " ('frito', 0.619145393371582),\n",
              " ('pescado', 0.6070436835289001),\n",
              " ('queso', 0.5999051928520203),\n",
              " ('papas', 0.5840942859649658),\n",
              " ('ensalada', 0.5787265300750732),\n",
              " ('empanadas', 0.5746084451675415),\n",
              " ('fritas', 0.5707438588142395),\n",
              " ('comer', 0.5427852869033813)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "target_word = 'rose' # Enter your word string here\n",
        "target_word = 'sia'\n",
        "target_word = 'colaboratory'\n",
        "target_word = 'phone'\n",
        "target_word = 'pollo'\n",
        "# check the word is in the vocabulary of the model\n",
        "assert model.has_index_for(target_word), f\"The selected word, {target_word}, is not included in the model's vocabulary\"\n",
        "model.most_similar(target_word)\n",
        "assert small_model.has_index_for(target_word), f\"The selected word, {target_word}, is not included in the model's vocabulary\"\n",
        "small_model.most_similar(target_word)\n",
        "assert large_model.has_index_for(target_word), f\"The selected word, {target_word}, is not included in the model's vocabulary\"\n",
        "large_model.most_similar(target_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c9e1b0",
      "metadata": {
        "id": "43c9e1b0"
      },
      "source": [
        "## Problem 2. Word Analogy (10 pts)\n",
        "- Another interesting thing you can play with Word2Vec is word analogy\n",
        "- Word analogy is done by adding and subtracting the word vector\n",
        "- In the cell below, you can run an example like this\n",
        "    - ``analogy(model, 'man', 'king', 'woman')`` represents a question of \"man is to king as woman is to what?\"\n",
        "- Try with your own choice.\n",
        "- **In your report**, present at least **5** interesting examples of your choice\n",
        "    - You can include the failure case\n",
        "    - Describe what did you expect and why the result was interesting for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75a2f6a",
      "metadata": {
        "id": "b75a2f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb87d813-ff86-42b0-d555-777f99d711f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('lou', 0.8535557985305786),\n",
            " ('miss', 0.8278504014015198),\n",
            " ('brother', 0.821225643157959),\n",
            " ('loves', 0.8201982378959656),\n",
            " ('karen', 0.8104918003082275),\n",
            " ('hey', 0.8101755380630493),\n",
            " ('sara', 0.8078510165214539),\n",
            " ('louis', 0.8035870790481567),\n",
            " ('ask', 0.803026556968689),\n",
            " ('sister', 0.8012278079986572)]\n",
            "[('hey', 0.7999284863471985),\n",
            " ('hello', 0.7457252740859985),\n",
            " ('babe', 0.7325332760810852),\n",
            " ('see', 0.7279067635536194),\n",
            " ('you', 0.7214446663856506),\n",
            " ('nialler', 0.719868004322052),\n",
            " ('bby', 0.7198134660720825),\n",
            " ('me', 0.7193549275398254),\n",
            " ('meet', 0.718912661075592),\n",
            " ('miss', 0.7149678468704224)]\n",
            "[('hey', 0.6881510019302368),\n",
            " ('hello', 0.6177436113357544),\n",
            " ('heyy', 0.5907323360443115),\n",
            " ('heey', 0.5504828095436096),\n",
            " ('austin', 0.5467607378959656),\n",
            " ('heeey', 0.5397984385490417),\n",
            " ('nhi', 0.5378016829490662),\n",
            " ('niall', 0.5240762829780579),\n",
            " ('babe', 0.5226432085037231),\n",
            " ('liam', 0.5189626812934875)]\n",
            "[('hermano', 0.902376651763916),\n",
            " ('llamo', 0.8981847763061523),\n",
            " ('papá', 0.896862804889679),\n",
            " ('gracias', 0.8829635977745056),\n",
            " ('hermana', 0.8789054751396179),\n",
            " ('¡', 0.8733869194984436),\n",
            " ('oye', 0.872072160243988),\n",
            " ('novio', 0.8716904520988464),\n",
            " ('hermanito', 0.8711584806442261),\n",
            " ('abuelo', 0.8644095063209534)]\n",
            "[('science', 0.8712683916091919),\n",
            " ('studies', 0.8644903898239136),\n",
            " ('history', 0.8603267669677734),\n",
            " ('subjects', 0.8559366464614868),\n",
            " ('related', 0.8442202210426331),\n",
            " ('subject', 0.8425312638282776),\n",
            " ('language', 0.8387334942817688),\n",
            " ('mock', 0.8342287540435791),\n",
            " ('physics', 0.8342232704162598),\n",
            " ('teachers', 0.8293237090110779)]\n",
            "[('science', 0.8712683916091919),\n",
            " ('studies', 0.8644903898239136),\n",
            " ('history', 0.8603267669677734),\n",
            " ('subjects', 0.8559366464614868),\n",
            " ('related', 0.8442202210426331),\n",
            " ('subject', 0.8425312638282776),\n",
            " ('language', 0.8387334942817688),\n",
            " ('mock', 0.8342287540435791),\n",
            " ('physics', 0.8342232704162598),\n",
            " ('teachers', 0.8293237090110779)]\n",
            "[('btw', 0.8318029642105103),\n",
            " ('ko', 0.8029528260231018),\n",
            " ('yeh', 0.7929247617721558),\n",
            " ('ty', 0.786778450012207),\n",
            " ('okay', 0.7863034009933472),\n",
            " ('hai', 0.7860004305839539),\n",
            " ('nia', 0.7836003303527832),\n",
            " ('hehe', 0.7808502316474915),\n",
            " ('haha', 0.7795053124427795),\n",
            " ('reply', 0.7784061431884766)]\n",
            "[('hai', 0.7466226816177368),\n",
            " ('log', 0.730256974697113),\n",
            " ('ko', 0.7229424715042114),\n",
            " ('yeh', 0.7195954322814941),\n",
            " ('ka', 0.7100127339363098),\n",
            " ('btw', 0.703843891620636),\n",
            " ('noh', 0.693363606929779),\n",
            " ('pe', 0.6884675025939941),\n",
            " ('okay', 0.6858369708061218),\n",
            " ('din', 0.684161365032196)]\n",
            "[('hello', 0.6588417887687683),\n",
            " ('hai', 0.5812368392944336),\n",
            " ('hey', 0.5804206728935242),\n",
            " ('btw', 0.5661873817443848),\n",
            " ('sir', 0.5632667541503906),\n",
            " ('nhi', 0.5539934635162354),\n",
            " ('ka', 0.553463339805603),\n",
            " ('ko', 0.5507255792617798),\n",
            " ('dear', 0.547335684299469),\n",
            " ('kay', 0.5437292456626892)]\n",
            "[('nexus', 0.9015675187110901),\n",
            " ('bmw', 0.8577726483345032),\n",
            " ('samsung', 0.8574326634407043),\n",
            " ('galaxy', 0.8553578853607178),\n",
            " ('sony', 0.8535411953926086),\n",
            " ('gb', 0.8520062565803528),\n",
            " ('windows', 0.8338016271591187),\n",
            " ('microsoft', 0.8307204246520996),\n",
            " ('playstation', 0.8292176127433777),\n",
            " ('lumia', 0.824989378452301)]\n",
            "[('samsung', 0.7474200129508972),\n",
            " ('led', 0.7440420985221863),\n",
            " ('sony', 0.7372706532478333),\n",
            " ('windows', 0.7351928353309631),\n",
            " ('galaxy', 0.7322561740875244),\n",
            " ('macbook', 0.7292993068695068),\n",
            " ('nexus', 0.7270244359970093),\n",
            " ('microsoft', 0.7262037992477417),\n",
            " ('nokia', 0.7174193859100342),\n",
            " ('blue', 0.7134707570075989)]\n",
            "[('red', 0.6179912686347961),\n",
            " ('blue', 0.6107025742530823),\n",
            " ('black', 0.5799776911735535),\n",
            " ('green', 0.577170729637146),\n",
            " ('white', 0.5711843371391296),\n",
            " ('samsung', 0.5673747062683105),\n",
            " ('iphone', 0.5625194311141968),\n",
            " ('microsoft', 0.5575581789016724),\n",
            " ('purple', 0.5469393134117126),\n",
            " ('silver', 0.5400528311729431)]\n"
          ]
        }
      ],
      "source": [
        "def analogy(model, x1, x2, y1):\n",
        "  pp.pprint(model.most_similar([x2, y1], negative=[x1]))\n",
        "def analogy(small_model, x1, x2, y1):\n",
        "  pp.pprint(small_model.most_similar([x2, y1], negative=[x1]))\n",
        "def analogy(large_model, x1, x2, y1):\n",
        "  pp.pprint(large_model.most_similar([x2, y1], negative=[x1]))\n",
        "\n",
        "# Try with your own word choice\n",
        "analogy(model, 'english', 'hi', 'spanish')\n",
        "analogy(small_model, 'english', 'hi', 'spanish')\n",
        "analogy(large_model, 'english', 'hi', 'spanish')\n",
        "analogy(model, 'english', 'hola', 'spanish')\n",
        "analogy(model, 'hola', 'english', 'spanish')\n",
        "analogy(model, 'hola', 'spanish', 'english')\n",
        "analogy(model, 'spanish', 'hi', 'english')\n",
        "analogy(small_model, 'spanish', 'hi', 'english')\n",
        "analogy(large_model, 'spanish', 'hi', 'english')\n",
        "analogy(model, 'banana', 'yellow', 'apple')\n",
        "analogy(small_model, 'banana', 'yellow', 'apple')\n",
        "analogy(large_model, 'banana', 'yellow', 'apple')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c29f5df",
      "metadata": {
        "id": "1c29f5df"
      },
      "source": [
        "## Problem 3. Simple Mathematics with Word2Vec (10 pts)\n",
        "- In this problem, you have to complete the given functions ``word_analogy_with_vector`` and ``get_cosine_similarity``\n",
        "- In your report, please include your code for these functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb11a51",
      "metadata": {
        "scrolled": true,
        "id": "dbb11a51"
      },
      "outputs": [],
      "source": [
        "def word_analogy_with_vector(model, x_1, x_2, y_1):\n",
        "  '''\n",
        "  This function takes a gensim Word2Vec model and outputs a vector to find y2 that corresponds to x_1 → x_2 == y_1 → y_2\n",
        "  e.g. x_1 (man) → x_2 (king) == y_1 (woman) → y_2(?)\n",
        "  \n",
        "  inputs\n",
        "  model (gensim.models.keyedvectors.KeyedVectors): Word2Vec model in KeyedVectors in gensim library\n",
        "  x_1, x_2, y_1 (str): Words in the model's vocabulary.\n",
        "  \n",
        "  output (np.ndarray): A vector in np.ndarray, which can be used to find proper y_2 for given (model, x_1, x_2, y_1)\n",
        "  '''\n",
        "  \n",
        "  # Write your code from here\n",
        "\n",
        "  return model[x_2]-model[x_1]+model[y_1]\n",
        "\n",
        "\n",
        "# test whether the function works well\n",
        "result_vector = word_analogy_with_vector(model, 'man', 'king', 'woman')\n",
        "print('result vector is ', result_vector)\n",
        "assert isinstance(result_vector, np.ndarray), \"Output of the function has to be np.ndarray\"\n",
        "model.most_similar(result_vector)\n",
        "\n",
        "'''\n",
        "Caution: The most_similar of this vector can include the input word such as 'king' on its top rank.\n",
        "The most similar vector for analogy('man', 'king', 'woman') is, in fact, still 'king'.\n",
        "But most_similar with positive=['king', 'woman'], negative=['man'] will automatically ignore these input words from the result,\n",
        "thus make 'queen' at the top\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd3e583",
      "metadata": {
        "id": "6dd3e583"
      },
      "outputs": [],
      "source": [
        "def get_cosine_similarity(model, x, y):\n",
        "  '''\n",
        "  This function returns cosine similarity of x,y\n",
        "  \n",
        "  inputs\n",
        "  model (gensim.models.keyedvectors.KeyedVectors): Word2Vec model in KeyedVectors in gensim library\n",
        "  x, y (str): Words in the model's vocabulary.\n",
        "  \n",
        "  output\n",
        "  similarity (float): cosine similarity between x's vector and y's vector\n",
        "  '''\n",
        "  # Write your codes from here\n",
        "  import numpy as np\n",
        "  ans=np.dot(model[x], model[y])/(np.linalg.norm(model[x])* np.linalg.norm(model[y]))\n",
        "  return ans\n",
        "\n",
        "# test the output with your own choice\n",
        "word_a = 'good'\n",
        "word_b = 'bad'\n",
        "\n",
        "similarity = get_cosine_similarity(model, word_a, word_b)\n",
        "print(similarity)\n",
        "assert -1 <= similarity <= 1, \"Similarity has to be between -1 and 1\"\n",
        "\n",
        "print('gensim library result:', model.similarity(word_a, word_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084dc056",
      "metadata": {
        "id": "084dc056"
      },
      "source": [
        "## Problem 4. Visualize Word Vectors (10 pts)\n",
        "- Select a list of words of your interest\n",
        "    - **At least 30 words for minimum** \n",
        "    - ``word_list`` is a list of strings\n",
        "    - every element in ``word_list`` has to be included in the model's vocabulary\n",
        "- Visualize the vectors of words using dimensionality reduction (in this case, PCA)\n",
        "- In your report, describe how words are located in 2D space\n",
        "    - How are the words clustered?\n",
        "    - Do you think the words are properly located based on their semantic meanings?\n",
        "    - Is there anything suprising or unexpected examples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c0d085",
      "metadata": {
        "id": "57c0d085"
      },
      "outputs": [],
      "source": [
        "# Run this cell to \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "  if len(words) < 30:\n",
        "    print(\"WARNING: For your report, please select more than 30 word samples for the visualization\")\n",
        "    print(f\"Current length of input word list: {len(words)}\")\n",
        "  word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "  twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "  for word, (x,y) in zip(words, twodim):\n",
        "      plt.text(x+0.05, y+0.05, word, fontsize=15)\n",
        "def display_pca_scatterplot1(small_model, words=None, sample=0):\n",
        "  if len(words) < 30:\n",
        "    print(\"WARNING: For your report, please select more than 30 word samples for the visualization\")\n",
        "    print(f\"Current length of input word list: {len(words)}\")\n",
        "  word_vectors = np.array([small_model[w] for w in words])\n",
        "\n",
        "  twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "  for word, (x,y) in zip(words, twodim):\n",
        "      plt.text(x+0.05, y+0.05, word, fontsize=15)\n",
        "def display_pca_scatterplot2(large_model, words=None, sample=0):\n",
        "  if len(words) < 30:\n",
        "    print(\"WARNING: For your report, please select more than 30 word samples for the visualization\")\n",
        "    print(f\"Current length of input word list: {len(words)}\")\n",
        "  word_vectors = np.array([large_model[w] for w in words])\n",
        "\n",
        "  twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "  plt.figure(figsize=(12,12))\n",
        "  plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "  for word, (x,y) in zip(words, twodim):\n",
        "      plt.text(x+0.05, y+0.05, word, fontsize=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f61a201",
      "metadata": {
        "id": "7f61a201"
      },
      "outputs": [],
      "source": [
        "# Select word list of your own interests\n",
        "word_list = [\"me\",\"love\",\"mom\",\"sister\",\"dad\",\"bed\",\"house\",\"family\",\"husband\",\"dream\",\"dog\",\"book\",\"peace\",\"social\",\"shower\",\"pizza\",\"cake\",\"workout\",\"fresh\",\"horse\",\"green\",\"rose\",\"sunny\",\"air\",\"peace\",\"diary\",\"mental\",\"china\",\"australia\",\"india\",\"korea\",\"taiwan\",\"yoga\",\"sofa\",\"flower\",\"nuts\",\"cereal\",\"cheese\",\"fries\",\"cookie\",\"dessert\",\"pumpkin\",\"breeze\",\"wooden\",\"bakery\",\"plants\",\"dense\",\"meditation\",\"newzealand\",\"fragrant\",\"seeds\"]\n",
        "display_pca_scatterplot(model, word_list)\n",
        "display_pca_scatterplot1(small_model, word_list)\n",
        "display_pca_scatterplot2(large_model, word_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03303d90",
      "metadata": {
        "id": "03303d90"
      },
      "source": [
        "## Problem 5. Compare different models (10 pts)\n",
        "- Word2Vec models can be trained on different corpus (text) or with different embedding size\n",
        "- The goal of this problem is to compare two different models with *different embedding size*\n",
        "- Select two models with different embedding size but the same training corpus\n",
        "    - e.g.  ``'glove-wiki-gigaword-50'``, ``'glove-wiki-gigaword-300'``\n",
        "\n",
        "- In your report, present at least **5** interesting examples that makes different result by model selection\n",
        "    - You can compare some word analogy examples or similairites or visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586ff05c",
      "metadata": {
        "id": "586ff05c"
      },
      "outputs": [],
      "source": [
        "# list the model name again if you want\n",
        "list(gensim.downloader.info()['models'].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a8f779",
      "metadata": {
        "scrolled": false,
        "id": "15a8f779"
      },
      "outputs": [],
      "source": [
        "# select your model code \n",
        "small_model_code = 'glove-twitter-50'\n",
        "large_model_code = 'glove-twitter-200'\n",
        "\n",
        "small_model = gensim.downloader.load(small_model_code)\n",
        "large_model = gensim.downloader.load(large_model_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b528bc3",
      "metadata": {
        "id": "5b528bc3"
      },
      "outputs": [],
      "source": [
        "# repeat problem 1,2,4 of your choice to compare small_model and large_model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "““AAT0320_Assignment1_ipynb”的副本.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}